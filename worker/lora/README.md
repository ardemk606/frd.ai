# LoRA Fine-tuning Module

## –û–±–∑–æ—Ä

–ú–æ–¥—É–ª—å `worker/lora/` –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –¥–æ–æ–±—É—á–µ–Ω–∏–µ LoRA-–∞–¥–∞–ø—Ç–µ—Ä–æ–≤ —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –±–∞–π–µ—Å–æ–≤—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞.

## –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã

### 1. `lora_tuning.py`
–û—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥—É–ª—å —Å –∫–ª–∞—Å—Å–æ–º `LoRATuner` –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –¥–æ–æ–±—É—á–µ–Ω–∏—è.

### 2. `bayesian_optimizer.py` 
–ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ LoRA —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Optuna.

### 3. `evaluation.py`
–°–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑:
- **BERTScore**: –ë—ã—Å—Ç—Ä–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞
- **LLM Judge**: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å

### 4. `lora_tuning_config.py`
–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–æ–æ–±—É—á–µ–Ω–∏—è.

## –ù–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π LLM Judge

### –ß—Ç–æ —Ç–∞–∫–æ–µ LLM Judge?

LLM Judge ‚Äî —ç—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –ø–æ–º–æ—â—å—é –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –û–Ω–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–º —É—Ä–æ–≤–Ω–µ, —á–µ–º –ø—Ä–æ—Å—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏.

### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ LLM Judge:
- üéØ **–ë–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞**: –£—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Å–µ–º–∞–Ω—Ç–∏–∫—É
- üß† **–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞**: –ü–æ–Ω–∏–º–∞–µ—Ç —Å—Ç–∏–ª—å –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∑–∞–¥–∞—á–µ
- üìä **–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏**: BERTScore (40%) + LLM Judge (60%)

### –ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ LLM Judge:
- ‚è∞ **–£–≤–µ–ª–∏—á–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è**: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ API-–≤—ã–∑–æ–≤—ã
- üí∞ **–ü–æ–≤—ã—à–µ–Ω–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–Ω–µ—à–Ω–∏—Ö LLM API
- üîÑ **–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å**: –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –≤–Ω–µ—à–Ω–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤

## –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —á–µ—Ä–µ–∑ API

### –ó–∞–ø—Ä–æ—Å —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ LLM Judge:

```json
{
  "project_id": 123,
  "fine_tuning_params": {
    "use_llm_judge": true,
    "judge_model_id": "gemini",
    "base_model_name": "Qwen/Qwen3-0.6B",
    "n_trials": 20
  }
}
```

### –ó–∞–ø—Ä–æ—Å –ë–ï–ó LLM Judge:

```json
{
  "project_id": 123,
  "fine_tuning_params": {
    "use_llm_judge": false,
    "base_model_name": "Qwen/Qwen3-0.6B",
    "n_trials": 15
  }
}
```

## –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —á–µ—Ä–µ–∑ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å

–í –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã –¥–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –∑–∞–ø—É—Å–∫–∞ fine-tuning:

### 1. üî• –ù–∞—Å—Ç—Ä–æ–∏—Ç—å Fine-tuning
–û—Ç–∫—Ä—ã–≤–∞–µ—Ç –º–æ–¥–∞–ª—å–Ω–æ–µ –æ–∫–Ω–æ —Å –æ–ø—Ü–∏—è–º–∏:
- ‚úÖ **–í–∫–ª—é—á–∏—Ç—å LLM Judge** (—á–µ–∫–±–æ–∫—Å)
- ü§ñ **–ú–æ–¥–µ–ª—å –¥–ª—è LLM Judge** (–≤—ã–±–æ—Ä –∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö)
- üéØ **–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å** (HuggingFace –Ω–∞–∑–≤–∞–Ω–∏–µ)
- üîÑ **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏** (5-100)

### 2. ‚ö° –ë—ã—Å—Ç—Ä—ã–π –∑–∞–ø—É—Å–∫
–ó–∞–ø—É—Å–∫–∞–µ—Ç —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é:
- LLM Judge: –≤–∫–ª—é—á–µ–Ω
- –ú–æ–¥–µ–ª—å Judge: –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
- –ü–æ–ø—ã—Ç–∫–∏: 20

## –õ–æ–≥–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### –ü—Ä–∏–º–µ—Ä—ã –ª–æ–≥–æ–≤ —Å LLM Judge:
```
INFO - LLM Judge –≤–∫–ª—é—á–µ–Ω —Å –º–æ–¥–µ–ª—å—é: gemini
INFO - Trial 5: params={'rank': 32, 'lora_alpha': 64}, BERTScore=0.8234
INFO - LLM Judge —Å—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: 78.50 (–Ω–∞ 5 –ø—Ä–∏–º–µ—Ä–∞—Ö)
INFO - –î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: BERTScore=0.8234, LLM Judge=78.50, Combined=0.8004
```

### –ü—Ä–∏–º–µ—Ä—ã –ª–æ–≥–æ–≤ –ë–ï–ó LLM Judge:
```
INFO - LLM Judge –≤—ã–∫–ª—é—á–µ–Ω, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ BERTScore
INFO - Trial 5: params={'rank': 32, 'lora_alpha': 64}, BERTScore=0.8234
INFO - –î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (–±–µ–∑ LLM Judge): BERTScore=0.8234
```

## –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é

### –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM Judge:
- üéØ **–í—ã—Å–æ–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∫–∞—á–µ—Å—Ç–≤—É**
- üí∞ **–î–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –±—é–¥–∂–µ—Ç** –Ω–∞ API-–≤—ã–∑–æ–≤—ã
- ‚è∞ **–ù–µ—Ç –∂–µ—Å—Ç–∫–∏—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏**
- üé® **–í–∞–∂–Ω–∞ –æ—Ü–µ–Ω–∫–∞ —Å—Ç–∏–ª—è –∏ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏**

### –ö–æ–≥–¥–∞ –ù–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM Judge:
- ‚ö° **–ë—ã—Å—Ç—Ä–æ–µ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏–µ**
- üí∏ **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –±—é–¥–∂–µ—Ç**
- üîí **–°—Ç—Ä–æ–≥–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç–∏**
- üìä **–ü—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏ (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, etc.)**

## –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è:
```bash
# –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LoRA
LORA_MODEL_NAME=Qwen/Qwen3-0.6B
LORA_N_TRIALS=20

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
LORA_NUM_EPOCHS=3
LORA_BATCH_SIZE=4
LORA_LEARNING_RATE=0.0001
```

### –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è LLM Judge:
- `gemini` (Google Gemini)
- `gigachat` (–°–±–µ—Ä GigaChat)
- –õ—é–±—ã–µ –¥—Ä—É–≥–∏–µ, –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤ `shared.llm`

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ—Ü–µ–Ω–∫–∏

```
Validation Data ‚Üí Model Predictions
                       ‚Üì
               ‚îå‚îÄ‚îÄ‚îÄ BERTScore (–±—ã—Å—Ç—Ä–æ)
               ‚îÇ
               ‚îî‚îÄ‚îÄ‚îÄ LLM Judge (—Ç–æ—á–Ω–æ, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
                       ‚Üì
               Combined Score (—Ñ–∏–Ω–∞–ª—å–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞)
```

### –§–æ—Ä–º—É–ª–∞ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏:
- **–° LLM Judge**: `Combined = BERTScore * 0.4 + LLMJudge * 0.6`
- **–ë–µ–∑ LLM Judge**: `Combined = BERTScore * 1.0`

## Troubleshooting

### –û—à–∏–±–∫–∞ "LLM Judge –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞":
```python
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:
from shared.llm import get_model_by_id
model = get_model_by_id("your_model_id")
```

### –í—ã—Å–æ–∫–æ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:
- –£–º–µ–Ω—å—à–∏—Ç–µ `n_trials`
- –û—Ç–∫–ª—é—á–∏—Ç–µ LLM Judge
- –£–º–µ–Ω—å—à–∏—Ç–µ —Ä–∞–∑–º–µ—Ä validation –≤—ã–±–æ—Ä–∫–∏

### –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –±–µ–∑ LLM Judge:
- –£–≤–µ–ª–∏—á—å—Ç–µ `n_trials`
- –í–∫–ª—é—á–∏—Ç–µ LLM Judge
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ 

## –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è

```bash
# –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
LORA_MODEL_NAME=Qwen/Qwen3-0.6B
LORA_N_TRIALS=20

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
MLFLOW_ENABLED=true
MLFLOW_TRACKING_URI=http://localhost:5000
MLFLOW_EXPERIMENT_NAME=LoRA_Fine_Tuning
```

## –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π MLflow Integration üîß

### –ß—Ç–æ —Ç–∞–∫–æ–µ MLflow?

MLflow ‚Äî —ç—Ç–æ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∂–∏–∑–Ω–µ–Ω–Ω—ã–º —Ü–∏–∫–ª–æ–º ML-—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤. –û–Ω–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç:

- üìä **–û—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏** –∫–∞–∂–¥–æ–≥–æ trial'–∞ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- üè∑Ô∏è **–°–æ—Ö—Ä–∞–Ω—è—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã** –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
- üéØ **–°—Ä–∞–≤–Ω–∏–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã** —Ä–∞–∑–Ω—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤
- üíæ **–•—Ä–∞–Ω–∏—Ç—å –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã** (–º–æ–¥–µ–ª–∏, –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, –ª–æ–≥–∏)
- üìà **–í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å** —á–µ—Ä–µ–∑ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ MLflow (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

#### 1. –õ–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ MLflow

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ MLflow
pip install mlflow>=2.8.0

# –ó–∞–ø—É—Å–∫ MLflow tracking server
mlflow server --host 0.0.0.0 --port 5000 --default-artifact-root ./mlruns

# MLflow UI –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É: http://localhost:5000
```

#### 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è

```bash
# –í–∫–ª—é—á–µ–Ω–∏–µ MLflow
export MLFLOW_ENABLED=true

# URL MLflow tracking server
export MLFLOW_TRACKING_URI=http://localhost:5000

# –ù–∞–∑–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
export MLFLOW_EXPERIMENT_NAME="LoRA_Fine_Tuning"
```

#### 3. Docker Compose –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

–î–æ–±–∞–≤—å—Ç–µ –≤ `docker-compose.yml`:

```yaml
services:
  mlflow:
    image: python:3.11-slim
    ports:
      - "5000:5000"
    volumes:
      - ./mlruns:/mlruns
      - ./mlflow_artifacts:/mlflow_artifacts
    command: >
      bash -c "
        pip install mlflow>=2.8.0 &&
        mlflow server 
        --host 0.0.0.0 
        --port 5000 
        --default-artifact-root /mlflow_artifacts
        --backend-store-uri sqlite:///mlflow.db
      "
    
  worker:
    environment:
      MLFLOW_ENABLED: "true"
      MLFLOW_TRACKING_URI: "http://mlflow:5000"
      MLFLOW_EXPERIMENT_NAME: "LoRA_Fine_Tuning"
    depends_on:
      - mlflow
```

### –ß—Ç–æ –ª–æ–≥–∏—Ä—É–µ—Ç—Å—è –≤ MLflow?

#### –î–ª—è –∫–∞–∂–¥–æ–≥–æ Trial –±–∞–π–µ—Å–æ–≤—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:

- **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã LoRA**: `rank`, `lora_alpha`, `lora_dropout`, `learning_rate`
- **–ú–µ—Ç—Ä–∏–∫–∏**: `bert_score`, `llm_judge_score`, `combined_score`  
- **–¢–µ–≥–∏**: `trial_number`, `optimization_type`, `model_type`
- **–ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã**: LoRA-–∞–¥–∞–ø—Ç–µ—Ä (–µ—Å–ª–∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω)

#### –î–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏:

- **–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã** –ø–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- **–§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏** (BERTScore + LLM Judge)
- **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏** (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ trials, –ª—É—á—à–∏–π score)
- **–§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å** –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### –ü—Ä–æ—Å–º–æ—Ç—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

1. **–û—Ç–∫—Ä–æ–π—Ç–µ MLflow UI**: http://localhost:5000
2. **–í—ã–±–µ—Ä–∏—Ç–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç**: `LoRA_Fine_Tuning`
3. **–°—Ä–∞–≤–Ω–∏–≤–∞–π—Ç–µ trial'—ã**: —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
4. **–ü—Ä–æ—Å–º–∞—Ç—Ä–∏–≤–∞–π—Ç–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã**: –º–æ–¥–µ–ª–∏, –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, –≥—Ä–∞—Ñ–∏–∫–∏
5. **–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Ç—Ä–µ–Ω–¥—ã**: –∫–∞–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–∞—é—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

### –û—Ç–∫–ª—é—á–µ–Ω–∏–µ MLflow

MLflow **–ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–ø—Ü–∏–æ–Ω–∞–ª–µ–Ω**. –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –Ω–µ–≥–æ:

```bash
# –ü–æ–ª–Ω–æ–µ –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ
export MLFLOW_ENABLED=false

# –ò–ª–∏ –ø—Ä–æ—Å—Ç–æ –Ω–µ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–π—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
unset MLFLOW_ENABLED
```

–ü—Ä–∏ –æ—Ç–∫–ª—é—á–µ–Ω–∏–∏ MLflow:
- ‚úÖ –í—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ fine-tuning —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ
- ‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –≤ –ª–æ–≥–∞—Ö
- ‚ùå –ù–µ—Ç –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
- ‚ùå –ù–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ 

## –†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º —Å –ø–∞–º—è—Ç—å—é –∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å—é üîß

### –ü—Ä–æ–±–ª–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ —Ä–µ—à–µ–Ω—ã:

#### 1. **–£—Ç–µ—á–∫–∏ –ø–∞–º—è—Ç–∏ PEFT –º–µ–∂–¥—É trial'–∞–º–∏**
**–ü—Ä–æ–±–ª–µ–º–∞**: PEFT –∞–¥–∞–ø—Ç–µ—Ä—ã –Ω–∞–∫–∞–ø–ª–∏–≤–∞–ª–∏—Å—å –≤ –ø–∞–º—è—Ç–∏ –º–µ–∂–¥—É trial'–∞–º–∏ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –≤—ã–∑—ã–≤–∞—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:
```
UserWarning: You are trying to modify a model with PEFT for a second time
UserWarning: Already found a `peft_config` attribute in the model
```

**–†–µ—à–µ–Ω–∏–µ**: 
- –î–æ–±–∞–≤–ª–µ–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è `safe_unload_peft_adapters()` –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –≤—ã–≥—Ä—É–∑–∫–∏ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤
- –§—É–Ω–∫—Ü–∏—è `cleanup_peft_model()` –¥–ª—è –ø–æ–ª–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ trial'–∞
- –£–ª—É—á—à–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Ä–∞–±–æ—Ç—ã —Å –∫–æ–ø–∏—è–º–∏ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏

#### 2. **Deprecated warning –¥–ª—è `tokenizer` –≤ Trainer**
**–ü—Ä–æ–±–ª–µ–º–∞**: 
```
FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
```

**–†–µ—à–µ–Ω–∏–µ**:
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–≥–æ API —á–µ—Ä–µ–∑ `inspect.signature()`
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ `processing_class` –≤ –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏—è—Ö transformers
- –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å `tokenizer` –≤ —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏—è—Ö
- –ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ deprecated warnings –∫–æ–≥–¥–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ

#### 3. **–ù–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏**
**–ü—Ä–æ–±–ª–µ–º–∞**: Memory leak –Ω–∞ GPU –º–µ–∂–¥—É trial'–∞–º–∏

**–†–µ—à–µ–Ω–∏–µ**:
- –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏ —Å `torch.cuda.empty_cache()` –∏ `torch.cuda.synchronize()`
- –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ CPU –ø–µ—Ä–µ–¥ —É–¥–∞–ª–µ–Ω–∏–µ–º
- –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞ —Å `gc.collect()`

### –ù–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:

```python
def safe_unload_peft_adapters(model):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –≤—ã–≥—Ä—É–∂–∞–µ—Ç PEFT –∞–¥–∞–ø—Ç–µ—Ä—ã –∏–∑ –º–æ–¥–µ–ª–∏"""
    # –ü—Ä–æ–±—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –≤—ã–≥—Ä—É–∑–∫–∏:
    # - unload_and_optionally_merge()
    # - merge_and_unload() 
    # - unload()
    # - disable_adapters()
    # - –ü—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ base_model
    
def cleanup_peft_model(peft_model):
    """–ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ PEFT –º–æ–¥–µ–ª–∏ –∏ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏"""
    # - –í—ã–≥—Ä—É–∑–∫–∞ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤
    # - –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –Ω–∞ CPU
    # - –£–¥–∞–ª–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫
    # - –û—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏
    # - –°–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
```

### –£–ª—É—á—à–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ objective_function:

```python
def objective_function(self, trial) -> float:
    # 1. –°–æ–∑–¥–∞–µ–º —á–∏—Å—Ç—É—é –∫–æ–ø–∏—é –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (–Ω–µ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º self.model)
    current_model = self.model
    
    # 2. –ë–µ–∑–æ–ø–∞—Å–Ω–æ –≤—ã–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ PEFT –∞–¥–∞–ø—Ç–µ—Ä—ã
    if hasattr(current_model, 'peft_config'):
        current_model = safe_unload_peft_adapters(current_model)
    
    # 3. –û–±—É—á–∞–µ–º —Å —á–∏—Å—Ç–æ–π –º–æ–¥–µ–ª—å—é
    peft_model = fine_tune_lora(current_model, ...)
    
    # 4. –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ: –ø–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–æ—Å–ª–µ trial'–∞
    finally:
        if peft_model is not None:
            cleanup_peft_model(peft_model)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
        torch.cuda.empty_cache()
        gc.collect()
```

### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π:

```bash
# –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –≤ worker/lora/
cd worker/lora/
python test_memory_management.py
```

–¢–µ—Å—Ç –ø—Ä–æ–≤–µ—Ä—è–µ—Ç:
- ‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å API Trainer
- ‚úÖ –ö–æ—Ä—Ä–µ–∫—Ç–Ω—É—é –æ—á–∏—Å—Ç–∫—É –ø–∞–º—è—Ç–∏ –º–µ–∂–¥—É trial'–∞–º–∏  
- ‚úÖ –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
- ‚úÖ –†–∞–±–æ—Ç—É —Ñ—É–Ω–∫—Ü–∏–π –≤—ã–≥—Ä—É–∑–∫–∏ PEFT –∞–¥–∞–ø—Ç–µ—Ä–æ–≤

### –†–µ–∑—É–ª—å—Ç–∞—Ç:

–ü–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π:
- ‚ùå **–£—Å—Ç—Ä–∞–Ω–µ–Ω—ã** –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö PEFT –∞–¥–∞–ø—Ç–µ—Ä–∞—Ö
- ‚ùå **–£—Å—Ç—Ä–∞–Ω–µ–Ω—ã** deprecated warnings –¥–ª—è `tokenizer`
- ‚úÖ **–°—Ç–∞–±–∏–ª—å–Ω–æ–µ** –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –º–µ–∂–¥—É trial'–∞–º–∏
- ‚úÖ **–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å** —Å —Ä–∞–∑–Ω—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ transformers
- ‚úÖ **–ù–∞–¥–µ–∂–Ω–∞—è** –±–∞–π–µ—Å–æ–≤—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–µ–∑ —Å–±–æ–µ–≤

### –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏:

–î–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –ø–∞–º—è—Ç–∏ –≤ production:

```python
# –í –ª–æ–≥–∞—Ö —Ç–µ–ø–µ—Ä—å –≤–∏–¥–Ω–æ:
logger.debug(f"Trial {trial.number}: –ø–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞")
logger.info("–í—ã–≥—Ä—É–∂–∞–µ–º PEFT –∞–¥–∞–ø—Ç–µ—Ä—ã –º–µ—Ç–æ–¥–æ–º unload_and_optionally_merge()")
logger.debug("PEFT –º–æ–¥–µ–ª—å –æ—á–∏—â–µ–Ω–∞ –∏–∑ –ø–∞–º—è—Ç–∏")
```

–ü—Ä–∏ –æ—Ç–∫–ª—é—á–µ–Ω–∏–∏ MLflow:
- ‚úÖ –í—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ fine-tuning —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ
- ‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –≤ –ª–æ–≥–∞—Ö
- ‚ùå –ù–µ—Ç –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
- ‚ùå –ù–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ 
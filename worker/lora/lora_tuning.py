import os
# –û—Ç–∫–ª—é—á–∞–µ–º JIT –∫–æ–º–ø–∏–ª—è—Ü–∏—é CUDA —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å Python.h
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, TaskType
from typing import Dict, Any, List
import json
import tempfile

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ —Ç–æ–≥–æ –∂–µ –ø–∞–∫–µ—Ç–∞
try:
    from .bayesian_optimizer import BayesianOptimizer
    from .lora_tuning_config import LoRATuningConfig
except ImportError:
    # –ï—Å–ª–∏ –∑–∞–ø—É—Å–∫–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é, –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω–æ
    from lora.bayesian_optimizer import BayesianOptimizer
    from lora.lora_tuning_config import LoRATuningConfig


def load_model(model_name: str = "mistralai/Mistral-7B-Instruct-v0.3"):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å
    
    Args:
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
    """
    
    print(f"üî• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        print("üìù –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º pad_token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            print("‚ö†Ô∏è –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω pad_token = eos_token")
        
        print("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        print("ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
        
        model = model.to(device)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ {device}")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        raise

def setup_lora_model(model, lora_params: Dict[str, Any]):
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç LoRA –¥–ª—è –º–æ–¥–µ–ª–∏"""
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=lora_params['rank'],
        lora_alpha=lora_params['lora_alpha'],
        lora_dropout=lora_params['lora_dropout'],
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
    )
    
    peft_model = get_peft_model(model, lora_config)
    return peft_model

def prepare_dataset(data: List[Dict[str, str]], tokenizer):
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    from datasets import Dataset
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è causal LM
    texts = []
    for item in data:
        text = f"<|im_start|>user\n{item['input']}<|im_end|>\n<|im_start|>assistant\n{item['output']}<|im_end|>"
        texts.append(text)
    
    # –°–æ–∑–¥–∞–µ–º Dataset
    dataset = Dataset.from_dict({"text": texts})
    
    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
    def tokenize_function(examples):
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        tokens = tokenizer(
            examples["text"], 
            truncation=True, 
            padding=True, 
            max_length=512
        )
        # –î–æ–±–∞–≤–ª—è–µ–º labels –¥–ª—è causal language modeling (–∫–æ–ø–∏—Ä—É–µ–º input_ids)
        tokens["labels"] = tokens["input_ids"]
        return tokens
    
    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=["text"])
    return tokenized_dataset

def fine_tune_lora(model, tokenizer, dataset, lora_params: Dict[str, Any], output_dir: str):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç LoRA –¥–æ–æ–±—É—á–µ–Ω–∏–µ"""
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º LoRA
    peft_model = setup_lora_model(model, lora_params)
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,
        per_device_train_batch_size=2,  # –£–º–µ–Ω—å—à–∏–ª batch size –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        gradient_accumulation_steps=4,  # –£–≤–µ–ª–∏—á–∏–ª grad accumulation
        learning_rate=lora_params['learning_rate'],
        logging_steps=10,
        save_steps=500,
        eval_steps=500,
        warmup_steps=100,
        remove_unused_columns=False,
        dataloader_drop_last=True,  # –ò–∑–±–µ–≥–∞–µ–º –ø—Ä–æ–±–ª–µ–º —Å —Ä–∞–∑–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏ –±–∞—Ç—á–µ–π
        fp16=True,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º mixed precision –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    )
    
    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä
    trainer = Trainer(
        model=peft_model,
        args=training_args,
        train_dataset=dataset,
        tokenizer=tokenizer,
    )
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
    trainer.train()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–¥–∞–ø—Ç–µ—Ä
    peft_model.save_pretrained(output_dir)
    
    return peft_model


class LoRATuner:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è LoRA —Å –±–∞–π–µ—Å–æ–≤—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
    
    def __init__(self, config: LoRATuningConfig = None):
        """
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è
        """
        self.config = config or LoRATuningConfig.from_env()
        self.optimizer = BayesianOptimizer(n_trials=self.config.n_trials)
        self.model = None
        self.tokenizer = None
        self.dataset = None
        
    def load_data(self, data_path: str, model_name: str = None) -> None:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            data_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        """
        print(f"üìñ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {data_path}...")
        
        try:
            with open(data_path, 'r', encoding='utf-8') as f:
                data = [json.loads(line) for line in f]
            
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ {data_path}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            print("ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
            model_to_use = model_name or self.config.model_name
            self.model, self.tokenizer = load_model(model_to_use)
            print("‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            print("üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
            self.dataset = prepare_dataset(data, self.tokenizer)
            print("‚úÖ –î–∞—Ç–∞—Å–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
            raise
        
    def objective_function(self, trial) -> float:
        """–¶–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –±–∞–π–µ—Å–æ–≤—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        lora_params = self.optimizer.suggest_lora_params(trial)
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–¥–∞–ø—Ç–µ—Ä–∞
        with tempfile.TemporaryDirectory() as temp_dir:
            try:
                # –î–æ–æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
                fine_tune_lora(
                    self.model, 
                    self.tokenizer, 
                    self.dataset, 
                    lora_params, 
                    temp_dir
                )
                
                # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
                # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª—É—á–∞–π–Ω—É—é –º–µ—Ç—Ä–∏–∫—É –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
                import random
                score = random.uniform(0.7, 0.95)
                
                print(f"Trial {trial.number}: params={lora_params}, score={score:.4f}")
                return score
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –≤ trial {trial.number}: {e}")
                return 0.0
    
    def run_optimization(self, data_path: str, output_dir: str = "./lora_results", model_name: str = None) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è
        
        Args:
            data_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        """
        print("–ù–∞—á–∏–Ω–∞–µ–º –¥–æ–æ–±—É—á–µ–Ω–∏–µ LoRA —Å –±–∞–π–µ—Å–æ–≤—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        self.load_data(data_path, model_name)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –±–∞–π–µ—Å–æ–≤—Å–∫—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
        print(f"–ó–∞–ø—É—Å–∫–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –Ω–∞ {self.config.n_trials} –ø–æ–ø—ã—Ç–æ–∫...")
        best_params = self.optimizer.optimize(self.objective_function)
        
        print(f"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {best_params}")
        
        # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        os.makedirs(output_dir, exist_ok=True)
        
        # –û–±—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        print("–û–±—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...")
        final_model = fine_tune_lora(
            self.model,
            self.tokenizer,
            self.dataset,
            best_params,
            output_dir
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = {
            'best_params': best_params,
            'output_dir': output_dir,
            'n_trials': self.config.n_trials
        }
        
        with open(os.path.join(output_dir, 'optimization_results.json'), 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"–î–æ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_dir}")
        return results


if __name__ == "__main__":
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è –ø—Ä—è–º–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –¥–æ–æ–±—É—á–µ–Ω–∏—è LoRA"""
    import argparse
    import sys
    import traceback
    
    print("üöÄ –ó–∞–ø—É—Å–∫ –¥–æ–æ–±—É—á–µ–Ω–∏—è LoRA...")
    print(f"Python –≤–µ—Ä—Å–∏—è: {sys.version}")
    print(f"–ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏: {sys.argv}")
    
    try:
        parser = argparse.ArgumentParser(description="–ó–∞–ø—É—Å–∫ –¥–æ–æ–±—É—á–µ–Ω–∏—è LoRA —Å –±–∞–π–µ—Å–æ–≤—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π")
        parser.add_argument("--data", type=str, required=True, 
                           help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (JSONL)")
        parser.add_argument("--output", type=str, default="./lora_results",
                           help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        parser.add_argument("--trials", type=int, default=5,
                           help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –±–∞–π–µ—Å–æ–≤—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏")
        parser.add_argument("--model", type=str, default="mistralai/Mistral-7B-Instruct-v0.3",
                           help="–ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è. –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã: "
                                "mistralai/Mistral-7B-Instruct-v0.3, "
                                "microsoft/DialoGPT-medium, "
                                "meta-llama/Llama-2-7b-chat-hf, "
                                "HuggingFaceH4/zephyr-7b-beta. "
                                "–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: mistralai/Mistral-7B-Instruct-v0.3")
        
        print("üìã –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤...")
        args = parser.parse_args()
        print(f"‚úÖ –ê—Ä–≥—É–º–µ–Ω—Ç—ã –ø–æ–ª—É—á–µ–Ω—ã: data={args.data}, output={args.output}, trials={args.trials}, model={args.model}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
        print(f"üìÇ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–∞ –¥–∞–Ω–Ω—ã—Ö: {args.data}")
        if not os.path.exists(args.data):
            print(f"‚ùå –û–®–ò–ë–ö–ê: –§–∞–π–ª {args.data} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            sys.exit(1)
        else:
            print(f"‚úÖ –§–∞–π–ª –Ω–∞–π–¥–µ–Ω: {args.data}")
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        print("‚öôÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏...")
        config = LoRATuningConfig.from_env()
        config.n_trials = args.trials
        print(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞: trials={config.n_trials}")
        
        # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º —Ç—é–Ω–µ—Ä
        print("ü§ñ –°–æ–∑–¥–∞–Ω–∏–µ LoRATuner...")
        tuner = LoRATuner(config)
        print("‚úÖ LoRATuner —Å–æ–∑–¥–∞–Ω")
        
        print("üéØ –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...")
        results = tuner.run_optimization(args.data, args.output, args.model)
        
        print("‚úÖ –î–æ–æ–±—É—á–µ–Ω–∏–µ LoRA —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üìä –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {results['best_params']}")
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {results['output_dir']}")
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except ImportError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
        print("üí° –í–æ–∑–º–æ–∂–Ω–æ, –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏")
        traceback.print_exc()
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        print("üîç –ü–æ–ª–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞:")
        traceback.print_exc()
        sys.exit(1) 
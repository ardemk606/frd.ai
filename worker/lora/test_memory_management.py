#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é PEFT –º–µ–∂–¥—É trial'–∞–º–∏
"""

import torch
import os
import json
import tempfile
import logging
from pathlib import Path

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_peft_memory_cleanup():
    """–¢–µ—Å—Ç –æ—á–∏—Å—Ç–∫–∏ –ø–∞–º—è—Ç–∏ PEFT –º–µ–∂–¥—É trial'–∞–º–∏"""
    try:
        from lora_tuning import LoRATuner, LoRATuningConfig, safe_unload_peft_adapters, cleanup_peft_model
        from lora_tuning_config import LoRATuningConfig
        
        logger.info("üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é PEFT...")
        
        # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        test_data = [
            {
                "instruction": "–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å",
                "input": "–ß—Ç–æ —Ç–∞–∫–æ–µ Python?",
                "output": "Python - —ç—Ç–æ —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è"
            },
            {
                "instruction": "–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å", 
                "input": "–ß—Ç–æ —Ç–∞–∫–æ–µ LoRA?",
                "output": "LoRA - —ç—Ç–æ –º–µ—Ç–æ–¥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è"
            },
            {
                "instruction": "–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å",
                "input": "–ß—Ç–æ —Ç–∞–∫–æ–µ PEFT?", 
                "output": "PEFT - —ç—Ç–æ Parameter Efficient Fine-Tuning"
            }
        ]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:
            for item in test_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
            test_data_path = f.name
        
        try:
            # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
            config = LoRATuningConfig(
                model_name="Qwen/Qwen3-0.6B",
                num_train_epochs=1,
                per_device_train_batch_size=1,
                gradient_accumulation_steps=1,
                logging_steps=1,
                save_steps=100,
                eval_steps=100,
                warmup_steps=0,
                n_trials=3  # –¢–æ–ª—å–∫–æ 3 trial'–∞ –¥–ª—è —Ç–µ—Å—Ç–∞
            )
            
            # –°–æ–∑–¥–∞–µ–º tuner —Å –æ—Ç–∫–ª—é—á–µ–Ω–Ω—ã–º LLM Judge –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            tuner = LoRATuner(
                config=config,
                use_llm_judge=False,  # –û—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                enable_mlflow=False   # –û—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
            )
            
            logger.info("‚úÖ LoRATuner —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            tuner.load_data(test_data_path, system_prompt="–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.")
            logger.info("‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—á–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ GPU
            if torch.cuda.is_available():
                initial_memory = torch.cuda.memory_allocated()
                logger.info(f"üìä –ù–∞—á–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –ø–∞–º—è—Ç–∏: {initial_memory / 1024 / 1024:.2f} MB")
            
            # –°–∏–º—É–ª–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ trial'–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ—á–∏—Å—Ç–∫–∏ –ø–∞–º—è—Ç–∏
            memory_usage = []
            
            for trial_num in range(3):
                logger.info(f"üîÑ –¢–µ—Å—Ç–∏—Ä—É–µ–º Trial {trial_num}")
                
                # –°–æ–∑–¥–∞–µ–º mock trial object 
                class MockTrial:
                    def __init__(self, number):
                        self.number = number
                
                mock_trial = MockTrial(trial_num)
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º objective function (–æ–Ω–∞ –¥–æ–ª–∂–Ω–∞ –æ—á–∏—â–∞—Ç—å –ø–∞–º—è—Ç—å)
                try:
                    score = tuner.objective_function(mock_trial)
                    logger.info(f"‚úÖ Trial {trial_num} –∑–∞–≤–µ—Ä—à–µ–Ω, score: {score:.4f}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ trial'–∞
                    if torch.cuda.is_available():
                        current_memory = torch.cuda.memory_allocated()
                        memory_usage.append(current_memory)
                        logger.info(f"üìä –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ Trial {trial_num}: {current_memory / 1024 / 1024:.2f} MB")
                        
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ Trial {trial_num}: {e}")
                    
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
            if torch.cuda.is_available() and memory_usage:
                max_memory = max(memory_usage)
                min_memory = min(memory_usage)
                memory_growth = max_memory - min_memory
                
                logger.info(f"üìà –ê–Ω–∞–ª–∏–∑ –ø–∞–º—è—Ç–∏:")
                logger.info(f"   –ú–∏–Ω–∏–º—É–º: {min_memory / 1024 / 1024:.2f} MB")
                logger.info(f"   –ú–∞–∫—Å–∏–º—É–º: {max_memory / 1024 / 1024:.2f} MB")
                logger.info(f"   –†–æ—Å—Ç –ø–∞–º—è—Ç–∏: {memory_growth / 1024 / 1024:.2f} MB")
                
                if memory_growth < 100 * 1024 * 1024:  # –ú–µ–Ω–µ–µ 100 MB —Ä–æ—Å—Ç–∞
                    logger.info("‚úÖ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç—å—é —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
                else:
                    logger.warning(f"‚ö†Ô∏è –í–æ–∑–º–æ–∂–Ω–∞ —É—Ç–µ—á–∫–∞ –ø–∞–º—è—Ç–∏: —Ä–æ—Å—Ç {memory_growth / 1024 / 1024:.2f} MB")
            
            logger.info("‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            
        finally:
            # –û—á–∏—Å—Ç–∫–∞
            os.unlink(test_data_path)
            
    except ImportError as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
        logger.error("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—ã –∑–∞–ø—É—Å–∫–∞–µ—Ç–µ —Ç–µ—Å—Ç –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ worker/lora/")
        
    except Exception as e:
        logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)


def test_trainer_compatibility():
    """–¢–µ—Å—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Trainer API"""
    try:
        from transformers import Trainer
        import inspect
        
        logger.info("üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å Trainer API...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏–≥–Ω–∞—Ç—É—Ä—É Trainer
        trainer_signature = inspect.signature(Trainer.__init__)
        params = list(trainer_signature.parameters.keys())
        
        logger.info(f"üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã Trainer.__init__: {params}")
        
        if 'processing_class' in params:
            logger.info("‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä 'processing_class'")
        else:
            logger.info("‚ÑπÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞—Ä—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä 'tokenizer'")
            
        if 'tokenizer' in params:
            logger.info("‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –ø–∞—Ä–∞–º–µ—Ç—Ä 'tokenizer'")
        else:
            logger.warning("‚ö†Ô∏è –ü–∞—Ä–∞–º–µ—Ç—Ä 'tokenizer' –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            
        logger.info("‚úÖ –¢–µ—Å—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏: {e}")


if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π LoRA fine-tuning...")
    print("=" * 50)
    
    # –¢–µ—Å—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ API
    test_trainer_compatibility()
    
    print("-" * 50)
    
    # –¢–µ—Å—Ç —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å GPU –∏–ª–∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ)
    if torch.cuda.is_available():
        test_peft_memory_cleanup()
    else:
        logger.info("‚è≠Ô∏è GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é")
        logger.info("üí° –î–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—É—Å—Ç–∏—Ç–µ –Ω–∞ –º–∞—à–∏–Ω–µ —Å GPU")
    
    print("=" * 50)
    print("‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")
    print("üí° –ï—Å–ª–∏ —Ç–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏ —É—Å–ø–µ—à–Ω–æ, –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é") 